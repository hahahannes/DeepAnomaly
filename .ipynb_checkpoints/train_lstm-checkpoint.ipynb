{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers import Dense,Activation,Dropout,Input\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import Adam\n",
    "import keras.callbacks as cb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_encoders():\n",
    "    src_encoder = LabelEncoder()\n",
    "    dst_encoder = LabelEncoder()\n",
    "    type_encoder = LabelEncoder()\n",
    "    activity_encoder = LabelEncoder()\n",
    "    protocol_encoder = LabelEncoder()\n",
    "    t_endpoint_encoder = LabelEncoder()\n",
    "    \n",
    "    src_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    dst_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    type_encoder.classes_ = np.load('encoders/type.npy')\n",
    "    activity_encoder.classes_ = np.load('encoders/activity.npy')\n",
    "    protocol_encoder.classes_ = np.load('encoders/protocol.npy')\n",
    "    t_endpoint_encoder.classes_ = np.load('encoders/endpoint.npy')\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n",
    "\n",
    "def train_encoders(rucio_data, use_cache=True):\n",
    "    \n",
    "    if use_cache:\n",
    "        if os.path.isfile('encoders/ddm_rse_endpoints.npy') and os.path.isfile('encoders/activity.npy'):\n",
    "            print('using cached LabelEncoders for encoding data.....')\n",
    "            src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder=load_encoders()\n",
    "        else:\n",
    "            print('NO cache found')\n",
    "    else:\n",
    "        print('No cached encoders found ! Training Some New Ones using input data!')\n",
    "        src_encoder = LabelEncoder()\n",
    "        dst_encoder = LabelEncoder()\n",
    "        type_encoder = LabelEncoder()\n",
    "        activity_encoder = LabelEncoder()\n",
    "        protocol_encoder = LabelEncoder()\n",
    "        t_endpoint_encoder = LabelEncoder()\n",
    "\n",
    "        src_encoder.fit(rucio_data['src-rse'].unique())\n",
    "        dst_encoder.fit(rucio_data['dst-rse'].unique())\n",
    "        type_encoder.fit(rucio_data['src-type'].unique())\n",
    "        activity_encoder.fit(rucio_data['activity'].unique())\n",
    "        protocol_encoder.fit(rucio_data['protocol'].unique())\n",
    "        t_endpoint_encoder.fit(rucio_data['transfer-endpoint'].unique())\n",
    "\n",
    "        np.save('encoders/src.npy', src_encoder.classes_)\n",
    "        np.save('encoders/dst.npy', dst_encoder.classes_)\n",
    "        np.save('encoders/type.npy', type_encoder.classes_)\n",
    "        np.save('encoders/activity.npy', activity_encoder.classes_)\n",
    "        np.save('encoders/protocol.npy', protocol_encoder.classes_)\n",
    "        np.save('encoders/endpoint.npy', t_endpoint_encoder.classes_)\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(rucio_data, use_cache=True):\n",
    "    \n",
    "    fields_to_drop = ['account','reason','checksum-adler','checksum-md5','guid','request-id','transfer-id','tool-id',\n",
    "                      'transfer-link','name','previous-request-id','scope','src-url','dst-url', 'Unnamed: 0']\n",
    "    timestamps = ['started_at', 'submitted_at','transferred_at']\n",
    "\n",
    "    #DROP FIELDS , CHANGE TIME FORMAT, add dataetime index\n",
    "    rucio_data = rucio_data.drop(fields_to_drop, axis=1)\n",
    "    for timestamp in timestamps:\n",
    "        rucio_data[timestamp]= pd.to_datetime(rucio_data[timestamp], infer_datetime_format=True)\n",
    "    rucio_data['delay'] = rucio_data['started_at'] - rucio_data['submitted_at']\n",
    "    rucio_data['delay'] = rucio_data['delay'].astype('timedelta64[s]')\n",
    "    \n",
    "    rucio_data = rucio_data.sort_values(by='submitted_at')\n",
    "    \n",
    "    # Reindex data with 'submitted_at timestamp'\n",
    "    rucio_data.index = pd.DatetimeIndex(rucio_data['submitted_at'])\n",
    "    \n",
    "    #remove all timestamp columns\n",
    "    rucio_data = rucio_data.drop(timestamps, axis=1)\n",
    "    \n",
    "    # encode categorical data\n",
    " \n",
    "    if use_cache==True:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=True)\n",
    "    else:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=False)\n",
    "\n",
    "    rucio_data['src-rse'] = src_encoder.transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rescale_data(rucio_data, durations):\n",
    "    # Normalization\n",
    "    # using custom scaling parameters (based on trends of the following variables)\n",
    "\n",
    "    durations = durations / 1e3\n",
    "    rucio_data['bytes'] = rucio_data['bytes'] / 1e10\n",
    "    rucio_data['delay'] = rucio_data['delay'] / 1e5\n",
    "    rucio_data['src-rse'] = rucio_data['src-rse'] / 1e2\n",
    "    rucio_data['dst-rse'] = rucio_data['dst-rse'] / 1e2\n",
    "    \n",
    "    return rucio_data, durations\n",
    "\n",
    "def plot_graphs_and_rescale(data):\n",
    "    \n",
    "    durations = data['duration']\n",
    "    durations.plot()\n",
    "    plt.ylabel('durations(seconds)')\n",
    "    plt.show()\n",
    "\n",
    "    filesize = data['bytes']\n",
    "    filesize.plot(label='filesize(bytes)')\n",
    "    plt.ylabel('bytes')\n",
    "    plt.show()\n",
    "\n",
    "    delays = data['delay']\n",
    "    delays.plot(label='delay(seconds)')\n",
    "    plt.ylabel('delay')\n",
    "    plt.show()\n",
    "    \n",
    "    print('rescaling input continuous variables : filesizes, queue-times, transfer-durations')\n",
    "    data, byte_scaler, delay_scaler, duration_scaler = rescale_data(data)\n",
    "\n",
    "    plt.plot(data['bytes'], 'r', label='filesize')\n",
    "    plt.plot(data['duration'], 'y', label='durations')\n",
    "    plt.plot(data['delay'],'g', label='queue-time')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return data, byte_scaler, delay_scaler, duration_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_inputs(rucio_data,durations, num_timesteps=50):\n",
    "    \n",
    "    #slice_size = batch_size*num_timesteps\n",
    "    print(rucio_data.shape[0], durations.shape)\n",
    "    n_examples = rucio_data.shape[0]\n",
    "    n_batches = (n_examples - num_timesteps +1)\n",
    "    print('Total Data points for training/testing : {} of {} timesteps each.'.format(n_batches, num_timesteps))\n",
    "    \n",
    "    inputs=[]\n",
    "    outputs=[]\n",
    "    for i in range(0,n_batches):\n",
    "        v = rucio_data[i:i+num_timesteps]\n",
    "        w = durations[i+num_timesteps-1]\n",
    "        inputs.append(v)\n",
    "        outputs.append(w)\n",
    "    inputs = np.stack(inputs)\n",
    "    outputs = np.stack(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = '../' # Change this as you need.\n",
    "\n",
    "def get_rucio_files(path='../', n_files =100):\n",
    "    abspaths = []\n",
    "    for fn in os.listdir(path):\n",
    "        if 'atlas_rucio' in fn:\n",
    "            abspaths.append(os.path.abspath(os.path.join(path, fn)))\n",
    "    print(\"\\n Found : \".join(abspaths))\n",
    "    print('\\n total files found = {}'.format(len(abspaths)))\n",
    "    return abspaths\n",
    "\n",
    "def load_rucio_data(file, use_cache = True, limit=None):\n",
    "    print('reading : {}'.format(file))\n",
    "    data = pd.read_csv(file)\n",
    "    if limit != None:\n",
    "        data= data[950000: 950000+limit]\n",
    "        print('Limiting data size to {} '.format(limit))\n",
    "#     print(data)\n",
    "    print('preprocessing data... ')\n",
    "    data = preprocess_data(data)\n",
    "    print('Saving indices for later..')\n",
    "    indices = data.index\n",
    "    durations = data['duration']\n",
    "    data = data.drop(['duration'], axis=1)\n",
    "    data = data[['bytes', 'delay', 'activity', 'dst-rse', 'dst-type',\n",
    "                 'protocol', 'src-rse', 'src-type', 'transfer-endpoint']]\n",
    "    data, durations = rescale_data(data, durations)\n",
    "    data = data.as_matrix()\n",
    "    durations = durations.as_matrix()\n",
    "    return data, durations, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path='data/'\n",
    "# a= get_rucio_files(path=path)\n",
    "# x, y, indices = load_rucio_data(a[1], limit=5)\n",
    "\n",
    "# print(x ,'\\n', y, '\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x,y = prepare_model_inputs(x,y,num_timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def return_to_original(x, y, preds, index=None):\n",
    "    #print(x.shape, y.shape)\n",
    "    #print(x[0,1])\n",
    "    n_steps = x.shape[1]\n",
    "    #print(index[:n_steps])\n",
    "    #print(index[n_steps-1:])\n",
    "    index = index[n_steps-1:]\n",
    "    \n",
    "    cols = ['bytes', 'delay', 'activity', 'dst-rse', 'dst-type','protocol', 'src-rse', 'src-type', 'transfer-endpoint']\n",
    "    data = list(x[0])\n",
    "    for i in range(1,x.shape[0]):\n",
    "        data.append(x[i,n_steps-1,:])\n",
    "    \n",
    "    data = data[n_steps-1:]\n",
    "    #print(len(data))\n",
    "    data = pd.DataFrame(data, index=index, columns=cols)\n",
    "    data['bytes'] = data['bytes'] * 1e10\n",
    "    data['delay'] = data['delay'] * 1e5\n",
    "    data['src-rse'] = data['src-rse'] * 1e2\n",
    "    data['dst-rse'] = data['dst-rse'] * 1e2\n",
    "    \n",
    "    data = data.round().astype(int)\n",
    "    print(data.shape)\n",
    "    data = decode_labels(data)\n",
    "    data['duration'] = y\n",
    "    data['prediction'] = pred\n",
    "    data['duration'] = data['duration'] * 1e3\n",
    "    data['prediction'] = data['prediction'] * 1e3\n",
    "    \n",
    "    return data\n",
    "\n",
    "# return_to_original(x,y, index=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_labels(rucio_data):\n",
    "    src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = load_encoders()\n",
    "    \n",
    "    rucio_data['src-rse'] = src_encoder.inverse_transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.inverse_transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.inverse_transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.inverse_transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.inverse_transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.inverse_transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.inverse_transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data\n",
    "\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(num_timesteps=50, batch_size = 512, parallel=False):\n",
    "\n",
    "    model = Sequential()\n",
    "    layers = [512, 512, 512, 512, 128, 1]\n",
    "    \n",
    "    model.add(LSTM(layers[0], input_shape=(num_timesteps, 9), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[1], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[2], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(layers[3]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[4]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[5]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if parallel:\n",
    "        model = make_parallel(model,4)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    sns.set_context('poster')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    print(len(losses))\n",
    "    fig.show()\n",
    "\n",
    "def train_network(model=None,limit=None, data=None, epochs=1,n_timesteps=100, batch=128, path=\"data/\",parallel=True):\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_model(num_timesteps=n_timesteps, parallel=parallel)\n",
    "        history = LossHistory()\n",
    "            \n",
    "        checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "        print('model built and compiled !')\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "    try:\n",
    "        for i,file in enumerate(a):\n",
    "            print(\"Training on file :{}\".format(file))\n",
    "            x, y, indices = load_rucio_data(file, limit=limit)\n",
    "            print('\\n Data Loaded and preprocessed !!....')\n",
    "            x, y = prepare_model_inputs(x, y, num_timesteps=n_timesteps)\n",
    "            print('Data ready for training.')\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training model...')\n",
    "            if parallel:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch*4,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "            else:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "\n",
    "            print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "            score = model.evaluate(x, y, verbose=0)\n",
    "            print(\"Network's Residual training score [MSE]: {0} ; [in seconds]: {1}\".format(score,np.sqrt(score)))\n",
    "            print(\"Training on {} finished !!\".format(file))\n",
    "            print('\\n Saving model to disk..')\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/lstm_model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/lstm_model.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            print('plotting losses..')\n",
    "            plot_losses(history.losses)\n",
    "\n",
    "        print('Training Complete !!')\n",
    "        \n",
    "        return training, model, indices, history.losses\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            return model, history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_network(n_timesteps=100, batch=256, parallel =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_lstm():\n",
    "    json_file = open('models/lstm_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"models/lstm_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print('Model model compiled!!')\n",
    "    return loaded_model\n",
    "\n",
    "def evaluate_network(limit=None, n_timesteps=100, path=\"data/\",model=None):\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "\n",
    "    for i,file in enumerate(a):\n",
    "        print(\"Training on file :{}\".format(file))\n",
    "        x, y, indices = load_rucio_data(file, limit=limit)\n",
    "        print('\\n Data Loaded and preprocessed !!....')\n",
    "        x, y = prepare_model_inputs(x, y, num_timesteps=n_timesteps)\n",
    "        print('Data ready for Evaluation')\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            start_time = time.time()\n",
    "            print('making predictions...')\n",
    "            model = load_lstm()\n",
    "            predictions = model.predict(x)\n",
    "            end = time.time - start_time\n",
    "            print('Done !! in {} min'.format(end/60))\n",
    "            print('plotting graphs')\n",
    "\n",
    "            plt.plot(y, 'g')\n",
    "            plt.plot(predictions, 'y')\n",
    "            plt.show()\n",
    "\n",
    "            data = return_to_original(x, y, predictions, index=indices)\n",
    "            plt.plot(data['duration'], 'g')\n",
    "            plt.plot(data['prediction'], 'y')\n",
    "            plt.title('Network predictions')\n",
    "            plt.ylabel('durations in seconds')\n",
    "            plt.show()\n",
    "\n",
    "            data['mae'] = data['duration'] - data['prediction']\n",
    "            data['mae'].plot()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate_network(path='data/', limit = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a= get_rucio_files(path=path)\n",
    "# x, y, indices = load_rucio_data(a[2], limit=1000)\n",
    "# print('\\n Data Loaded and preprocessed !!....')\n",
    "# x, y = prepare_model_inputs(x, y, num_timesteps=100)\n",
    "# with tf.device('/gpu:0'):\n",
    "#     model = load_lstm()\n",
    "#     pred= model.predict(x)\n",
    "#     print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
