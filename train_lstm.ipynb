{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/releases/matplotlib/1.5.1-b2015/x86_64-slc6-gcc62-opt/lib/python3.5/site-packages/matplotlib-1.5.1-py3.5-linux-x86_64.egg/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/cvmfs/sft.cern.ch/lcg/releases/matplotlib/1.5.1-b2015/x86_64-slc6-gcc62-opt/lib/python3.5/site-packages/matplotlib-1.5.1-py3.5-linux-x86_64.egg/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers import Dense,Activation,Dropout,Input\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import Adam\n",
    "import keras.callbacks as cb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras_tqdm import TQDMNotebookCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_encoders():\n",
    "    src_encoder = LabelEncoder()\n",
    "    dst_encoder = LabelEncoder()\n",
    "    type_encoder = LabelEncoder()\n",
    "    activity_encoder = LabelEncoder()\n",
    "    protocol_encoder = LabelEncoder()\n",
    "    t_endpoint_encoder = LabelEncoder()\n",
    "    \n",
    "    src_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    dst_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    type_encoder.classes_ = np.load('encoders/type.npy')\n",
    "    activity_encoder.classes_ = np.load('encoders/activity.npy')\n",
    "    protocol_encoder.classes_ = np.load('encoders/protocol.npy')\n",
    "    t_endpoint_encoder.classes_ = np.load('encoders/endpoint.npy')\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n",
    "\n",
    "def train_encoders(rucio_data, use_cache=True):\n",
    "    \n",
    "    if use_cache:\n",
    "        if os.path.isfile('encoders/ddm_rse_endpoints.npy') and os.path.isfile('encoders/activity.npy'):\n",
    "            print('using cached LabelEncoders for encoding data.....')\n",
    "            src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder=load_encoders()\n",
    "        else:\n",
    "            print('NO cache found')\n",
    "    else:\n",
    "        print('No cached encoders found ! Training Some New Ones using input data!')\n",
    "        src_encoder = LabelEncoder()\n",
    "        dst_encoder = LabelEncoder()\n",
    "        type_encoder = LabelEncoder()\n",
    "        activity_encoder = LabelEncoder()\n",
    "        protocol_encoder = LabelEncoder()\n",
    "        t_endpoint_encoder = LabelEncoder()\n",
    "\n",
    "        src_encoder.fit(rucio_data['src-rse'].unique())\n",
    "        dst_encoder.fit(rucio_data['dst-rse'].unique())\n",
    "        type_encoder.fit(rucio_data['src-type'].unique())\n",
    "        activity_encoder.fit(rucio_data['activity'].unique())\n",
    "        protocol_encoder.fit(rucio_data['protocol'].unique())\n",
    "        t_endpoint_encoder.fit(rucio_data['transfer-endpoint'].unique())\n",
    "\n",
    "        np.save('encoders/src.npy', src_encoder.classes_)\n",
    "        np.save('encoders/dst.npy', dst_encoder.classes_)\n",
    "        np.save('encoders/type.npy', type_encoder.classes_)\n",
    "        np.save('encoders/activity.npy', activity_encoder.classes_)\n",
    "        np.save('encoders/protocol.npy', protocol_encoder.classes_)\n",
    "        np.save('encoders/endpoint.npy', t_endpoint_encoder.classes_)\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(rucio_data, use_cache=True):\n",
    "    \n",
    "    fields_to_drop = ['account','reason','checksum-adler','checksum-md5','guid','request-id','transfer-id','tool-id',\n",
    "                      'transfer-link','name','previous-request-id','scope','src-url','dst-url', 'Unnamed: 0']\n",
    "    timestamps = ['started_at', 'submitted_at','transferred_at']\n",
    "\n",
    "    #DROP FIELDS , CHANGE TIME FORMAT, add dataetime index\n",
    "    rucio_data = rucio_data.drop(fields_to_drop, axis=1)\n",
    "    for timestamp in timestamps:\n",
    "        rucio_data[timestamp]= pd.to_datetime(rucio_data[timestamp], infer_datetime_format=True)\n",
    "    rucio_data['delay'] = rucio_data['started_at'] - rucio_data['submitted_at']\n",
    "    rucio_data['delay'] = rucio_data['delay'].astype('timedelta64[s]')\n",
    "    \n",
    "    rucio_data = rucio_data.sort_values(by='submitted_at')\n",
    "    \n",
    "    # Reindex data with 'submitted_at timestamp'\n",
    "    rucio_data.index = pd.DatetimeIndex(rucio_data['submitted_at'])\n",
    "    \n",
    "    #remove all timestamp columns\n",
    "    rucio_data = rucio_data.drop(timestamps, axis=1)\n",
    "    \n",
    "    # encode categorical data\n",
    " \n",
    "    if use_cache==True:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=True)\n",
    "    else:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=False)\n",
    "\n",
    "    rucio_data['src-rse'] = src_encoder.transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_data(rucio_data, durations):\n",
    "    # Normalization\n",
    "    # using custom scaling parameters (based on trends of the following variables)\n",
    "\n",
    "    durations = durations / 1e3\n",
    "    rucio_data['bytes'] = rucio_data['bytes'] / 1e10\n",
    "    rucio_data['delay'] = rucio_data['delay'] / 1e5\n",
    "    rucio_data['src-rse'] = rucio_data['src-rse'] / 1e2\n",
    "    rucio_data['dst-rse'] = rucio_data['dst-rse'] / 1e2\n",
    "    \n",
    "    return rucio_data, durations\n",
    "\n",
    "def plot_graphs_and_rescale(data):\n",
    "    \n",
    "    durations = data['duration']\n",
    "    durations.plot()\n",
    "    plt.ylabel('durations(seconds)')\n",
    "    plt.show()\n",
    "\n",
    "    filesize = data['bytes']\n",
    "    filesize.plot(label='filesize(bytes)')\n",
    "    plt.ylabel('bytes')\n",
    "    plt.show()\n",
    "\n",
    "    delays = data['delay']\n",
    "    delays.plot(label='delay(seconds)')\n",
    "    plt.ylabel('delay')\n",
    "    plt.show()\n",
    "    \n",
    "    print('rescaling input continuous variables : filesizes, queue-times, transfer-durations')\n",
    "    data, byte_scaler, delay_scaler, duration_scaler = rescale_data(data)\n",
    "\n",
    "    plt.plot(data['bytes'], 'r', label='filesize')\n",
    "    plt.plot(data['duration'], 'y', label='durations')\n",
    "    plt.plot(data['delay'],'g', label='queue-time')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return data, byte_scaler, delay_scaler, duration_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_model_inputs(rucio_data,durations, num_timesteps=50):\n",
    "    \n",
    "    #slice_size = batch_size*num_timesteps\n",
    "    print(rucio_data.shape[0], durations.shape)\n",
    "    n_examples = rucio_data.shape[0]\n",
    "    n_batches = (n_examples - num_timesteps +1)\n",
    "    print('Total Data points for training/testing : {} of {} timesteps each.'.format(n_batches, num_timesteps))\n",
    "    \n",
    "    inputs=[]\n",
    "    outputs=[]\n",
    "    for i in range(0,n_batches):\n",
    "        v = rucio_data[i:i+num_timesteps]\n",
    "        w = durations[i+num_timesteps-1]\n",
    "        inputs.append(v)\n",
    "        outputs.append(w)\n",
    "    inputs = np.stack(inputs)\n",
    "    outputs = np.stack(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../' # Change this as you need.\n",
    "\n",
    "def get_rucio_files(path='../', n_files =100):\n",
    "    abspaths = []\n",
    "    for fn in os.listdir(path):\n",
    "        if 'atlas_rucio' in fn:\n",
    "            abspaths.append(os.path.abspath(os.path.join(path, fn)))\n",
    "    print(\"\\n Found : \".join(abspaths))\n",
    "    print('\\n total files found = {}'.format(len(abspaths)))\n",
    "    return abspaths\n",
    "\n",
    "def load_rucio_data(file, use_cache = True, limit=None):\n",
    "    print('reading : {}'.format(file))\n",
    "    data = pd.read_csv(file)\n",
    "    if limit != None:\n",
    "        data= data[:limit]\n",
    "        print('Limiting data size to {} '.format(limit))\n",
    "    print(data)\n",
    "    print('preprocess data... ')\n",
    "    data = preprocess_data(data)\n",
    "    print('Saving indices for later..')\n",
    "    indices = data.index\n",
    "    durations = data['duration']\n",
    "    data = data.drop(['duration'], axis=1)\n",
    "    data = data[['bytes', 'delay', 'activity', 'dst-rse', 'dst-type',\n",
    "                 'protocol', 'src-rse', 'src-type', 'transfer-endpoint']]\n",
    "    data, durations = rescale_data(data, durations)\n",
    "    \n",
    "    return data, durations, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eos/user/v/vysharma/atlas_rucio-events-2017.06.01.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.02.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.03.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.04.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.05.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.06.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.07.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.08.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.09.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.10.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.11.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.12.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.13.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.14.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.15.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.16.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.17.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.18.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.19.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.20.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.21.csv\n",
      " Found : /eos/user/v/vysharma/atlas_rucio-events-2017.06.22.csv\n",
      "\n",
      " total files found = 22\n",
      "reading : /eos/user/v/vysharma/atlas_rucio-events-2017.06.13.csv\n",
      "Limiting data size to 5 \n",
      "   Unnamed: 0  account          activity       bytes checksum-adler  \\\n",
      "0           0      NaN  Data Rebalancing       13453       1b2cee55   \n",
      "1           1      NaN  Data Rebalancing   461604239       a82b2ec7   \n",
      "2           2      NaN  Data Rebalancing      644194       90e5321d   \n",
      "3           3      NaN  Data Rebalancing     1532641       5d2e4462   \n",
      "4           4      NaN  Data Rebalancing  2111902672       2af98a90   \n",
      "\n",
      "   checksum-md5              dst-rse dst-type  \\\n",
      "0           NaN  IN2P3-CC_PHYS-HIGGS     DISK   \n",
      "1           NaN   IFIC-LCG2_DATADISK     DISK   \n",
      "2           NaN  IN2P3-CC_PHYS-HIGGS     DISK   \n",
      "3           NaN  IN2P3-CC_PHYS-HIGGS     DISK   \n",
      "4           NaN   IFIC-LCG2_DATADISK     DISK   \n",
      "\n",
      "                                             dst-url  duration  \\\n",
      "0  srm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/p...        12   \n",
      "1  srm://srmv2.ific.uv.es:8443/srm/managerv2?SFN=...        13   \n",
      "2  srm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/p...        15   \n",
      "3  srm://ccsrm.in2p3.fr:8443/srm/managerv2?SFN=/p...        10   \n",
      "4  srm://srmv2.ific.uv.es:8443/srm/managerv2?SFN=...       101   \n",
      "\n",
      "          ...                                  src-rse src-type  \\\n",
      "0         ...           UKI-SOUTHGRID-RALPP_PHYS-HIGGS     DISK   \n",
      "1         ...                             PIC_DATADISK     DISK   \n",
      "2         ...           UKI-SOUTHGRID-RALPP_PHYS-HIGGS     DISK   \n",
      "3         ...           UKI-SOUTHGRID-RALPP_PHYS-HIGGS     DISK   \n",
      "4         ...                           AGLT2_DATADISK     DISK   \n",
      "\n",
      "                                             src-url           started_at  \\\n",
      "0  srm://heplnx204.pp.rl.ac.uk:8443/srm/managerv2...  2017-06-13 04:47:47   \n",
      "1  srm://srmatlas.pic.es:8443/srm/managerv2?SFN=/...  2017-06-13 04:50:31   \n",
      "2  srm://heplnx204.pp.rl.ac.uk:8443/srm/managerv2...  2017-06-13 05:00:28   \n",
      "3  srm://heplnx204.pp.rl.ac.uk:8443/srm/managerv2...  2017-06-13 04:57:46   \n",
      "4  srm://head01.aglt2.org:8443/srm/managerv2?SFN=...  2017-06-13 05:08:42   \n",
      "\n",
      "          submitted_at         tool-id          transfer-endpoint  \\\n",
      "0  2017-06-12 09:13:22  rucio-conveyor  https://fts3.cern.ch:8446   \n",
      "1  2017-06-12 01:41:54  rucio-conveyor  https://fts3.cern.ch:8446   \n",
      "2  2017-06-12 09:13:25  rucio-conveyor  https://fts3.cern.ch:8446   \n",
      "3  2017-06-12 09:13:23  rucio-conveyor  https://fts3.cern.ch:8446   \n",
      "4  2017-06-12 01:33:26  rucio-conveyor  https://fts3.cern.ch:8446   \n",
      "\n",
      "                            transfer-id  \\\n",
      "0  0847d007-12bf-5a70-a0df-e55b9932684d   \n",
      "1  f79afcf5-c475-5b0e-a33b-2bb69d357d1a   \n",
      "2  7282d75c-a8ab-5af1-a109-484ce60079ef   \n",
      "3  6dc9c4db-2c45-5553-8165-46a7c99d16d2   \n",
      "4  1398e085-76e5-579f-bb7c-28954d4d67b6   \n",
      "\n",
      "                                       transfer-link       transferred_at  \n",
      "0  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/08...  2017-06-13 04:47:59  \n",
      "1  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/f7...  2017-06-13 04:50:44  \n",
      "2  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/72...  2017-06-13 05:00:43  \n",
      "3  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/6d...  2017-06-13 04:57:56  \n",
      "4  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/13...  2017-06-13 05:10:23  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "preprocess data... \n",
      "using cached LabelEncoders for encoding data.....\n",
      "Saving indices for later..\n",
      "                        bytes    delay  activity  dst-rse  dst-type  protocol  \\\n",
      "submitted_at                                                                    \n",
      "2017-06-12 01:33:26  0.211190  0.99316         2     2.93         0         2   \n",
      "2017-06-12 01:41:54  0.046160  0.97717         2     2.93         0         2   \n",
      "2017-06-12 09:13:22  0.000001  0.70465         2     3.22         0         2   \n",
      "2017-06-12 09:13:23  0.000153  0.71063         2     3.22         0         2   \n",
      "2017-06-12 09:13:25  0.000064  0.71223         2     3.22         0         2   \n",
      "\n",
      "                     src-rse  src-type  transfer-endpoint  \n",
      "submitted_at                                               \n",
      "2017-06-12 01:33:26     0.01         0                  3  \n",
      "2017-06-12 01:41:54     5.62         0                  3  \n",
      "2017-06-12 09:13:22     8.76         0                  3  \n",
      "2017-06-12 09:13:23     8.76         0                  3  \n",
      "2017-06-12 09:13:25     8.76         0                  3   \n",
      " submitted_at\n",
      "2017-06-12 01:33:26    0.101\n",
      "2017-06-12 01:41:54    0.013\n",
      "2017-06-12 09:13:22    0.012\n",
      "2017-06-12 09:13:23    0.010\n",
      "2017-06-12 09:13:25    0.015\n",
      "Name: duration, dtype: float64 \n",
      " DatetimeIndex(['2017-06-12 01:33:26', '2017-06-12 01:41:54',\n",
      "               '2017-06-12 09:13:22', '2017-06-12 09:13:23',\n",
      "               '2017-06-12 09:13:25'],\n",
      "              dtype='datetime64[ns]', name='submitted_at', freq=None)\n"
     ]
    }
   ],
   "source": [
    "a= get_rucio_files(path=path)\n",
    "x, y, indices = load_rucio_data(a[12], limit=5)\n",
    "\n",
    "print(x ,'\\n', y, '\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (5,)\n",
      "Total Data points for training/testing : 4 of 2 timesteps each.\n",
      "(4, 2, 9) (4,)\n"
     ]
    }
   ],
   "source": [
    "x,y = prepare_model_inputs(x,y,num_timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_to_original(x, y, index=None):\n",
    "    y = y * 1e3\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[0])\n",
    "    cols = ['bytes', 'delay', 'activity', 'dst-rse', 'dst-type','protocol', 'src-rse', 'src-type', 'transfer-endpoint']\n",
    "    n_steps = x.shape[1]\n",
    "    data = list(x[0])\n",
    "    for i in range(1,x.shape[0]):\n",
    "        data.append(x[i,n_steps-1,:])\n",
    "    data = pd.DataFrame(data, index=indices, columns=cols)\n",
    "    data['bytes'] = data['bytes'] * 1e10\n",
    "    data['delay'] = data['delay'] * 1e5\n",
    "    data['src-rse'] = data['src-rse'] * 1e2\n",
    "    data['dst-rse'] = data['dst-rse'] * 1e2\n",
    "    \n",
    "    data = data.round().astype(int)\n",
    "    data = decode_labels(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c92c170ae78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrucio_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLossHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cb' is not defined"
     ]
    }
   ],
   "source": [
    "def decode_labels(rucio_data):\n",
    "    src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = load_encoders()\n",
    "    \n",
    "    rucio_data['src-rse'] = src_encoder.inverse_transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.inverse_transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.inverse_transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.inverse_transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.inverse_transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.inverse_transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.inverse_transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data\n",
    "\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "            \n",
    "        return Model(input=model.inputs, output=merged)\n",
    "\n",
    "def build_model(num_timesteps=50, batch_size = 512, make_parallel=False):\n",
    "\n",
    "    model = Sequential()\n",
    "    layers = [512, 512, 512, 512, 128, 1]\n",
    "    \n",
    "    model.add(LSTM(layers[0], input_shape=(num_timesteps, 9), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[1], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[2], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(layers[3]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[4]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[5]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if make_parallel:\n",
    "        model = make_parallel(model,4)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    sns.set_context('poster')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    print(len(losses))\n",
    "    fig.show()\n",
    "\n",
    "def train_network(model=None,limit=None, data=None, epochs=1,n_timesteps=100, batch=128):\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_model(num_timesteps=n_timesteps, make_parallel=True)\n",
    "        history = LossHistory()\n",
    "            \n",
    "        checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "        print('model built and compiled !')\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "    try:\n",
    "        for i,file in enumerate(a):\n",
    "            print(\"Training on file :{}\".format(file))\n",
    "            x, y, indices = load_rucio_data(file, limit=None)\n",
    "            print('\\n Data Loaded and preprocessed !!....')\n",
    "            x,y = prepare_model_inputs(x,y,num_timesteps=n_timesteps)\n",
    "            print('Data ready for training.')\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training model...')\n",
    "            training = model.fit(x, y, epochs=epochs, batch_size=batch,\n",
    "                                 validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                 verbose=0)\n",
    "\n",
    "            print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "            score = model.evaluate(x, y, verbose=0)\n",
    "            print(\"Network's Residual training score [MSE]: {0} ; [in seconds]: {1}\".format(score,np.sqrt(score)))\n",
    "            print(\"Training on {} finished !!\".format(file))\n",
    "            print('\\n Saving model to disk..')\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/lstm_model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/lstm_model.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            print('plotting losses..')\n",
    "            plot_losses(history.losses)\n",
    "\n",
    "        print('Training Complete !!')\n",
    "        \n",
    "        return training, model, indices, history.losses\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            return model, history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
