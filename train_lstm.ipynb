{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers import Dense,Activation,Dropout,Input\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import Adam\n",
    "import keras.callbacks as cb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_encoders():\n",
    "    src_encoder = LabelEncoder()\n",
    "    dst_encoder = LabelEncoder()\n",
    "    type_encoder = LabelEncoder()\n",
    "    activity_encoder = LabelEncoder()\n",
    "    protocol_encoder = LabelEncoder()\n",
    "    t_endpoint_encoder = LabelEncoder()\n",
    "    \n",
    "    src_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    dst_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    type_encoder.classes_ = np.load('encoders/type.npy')\n",
    "    activity_encoder.classes_ = np.load('encoders/activity.npy')\n",
    "    protocol_encoder.classes_ = np.load('encoders/protocol.npy')\n",
    "    t_endpoint_encoder.classes_ = np.load('encoders/endpoint.npy')\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n",
    "\n",
    "def train_encoders(rucio_data, use_cache=True):\n",
    "    \n",
    "    if use_cache:\n",
    "        if os.path.isfile('encoders/ddm_rse_endpoints.npy') and os.path.isfile('encoders/activity.npy'):\n",
    "            print('using cached LabelEncoders for encoding data.....')\n",
    "            src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder=load_encoders()\n",
    "        else:\n",
    "            print('NO cache found')\n",
    "    else:\n",
    "        print('No cached encoders found ! Training Some New Ones using input data!')\n",
    "        src_encoder = LabelEncoder()\n",
    "        dst_encoder = LabelEncoder()\n",
    "        type_encoder = LabelEncoder()\n",
    "        activity_encoder = LabelEncoder()\n",
    "        protocol_encoder = LabelEncoder()\n",
    "        t_endpoint_encoder = LabelEncoder()\n",
    "\n",
    "        src_encoder.fit(rucio_data['src-rse'].unique())\n",
    "        dst_encoder.fit(rucio_data['dst-rse'].unique())\n",
    "        type_encoder.fit(rucio_data['src-type'].unique())\n",
    "        activity_encoder.fit(rucio_data['activity'].unique())\n",
    "        protocol_encoder.fit(rucio_data['protocol'].unique())\n",
    "        t_endpoint_encoder.fit(rucio_data['transfer-endpoint'].unique())\n",
    "\n",
    "        np.save('encoders/src.npy', src_encoder.classes_)\n",
    "        np.save('encoders/dst.npy', dst_encoder.classes_)\n",
    "        np.save('encoders/type.npy', type_encoder.classes_)\n",
    "        np.save('encoders/activity.npy', activity_encoder.classes_)\n",
    "        np.save('encoders/protocol.npy', protocol_encoder.classes_)\n",
    "        np.save('encoders/endpoint.npy', t_endpoint_encoder.classes_)\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(rucio_data, use_cache=True):\n",
    "    \n",
    "    fields_to_drop = ['account','reason','checksum-adler','checksum-md5','guid','request-id','transfer-id','tool-id',\n",
    "                      'transfer-link','name','previous-request-id','scope','src-url','dst-url', 'Unnamed: 0']\n",
    "    timestamps = ['started_at', 'submitted_at','transferred_at']\n",
    "\n",
    "    #DROP FIELDS , CHANGE TIME FORMAT, add dataetime index\n",
    "    rucio_data = rucio_data.drop(fields_to_drop, axis=1)\n",
    "    for timestamp in timestamps:\n",
    "        rucio_data[timestamp]= pd.to_datetime(rucio_data[timestamp], infer_datetime_format=True)\n",
    "    rucio_data['delay'] = rucio_data['started_at'] - rucio_data['submitted_at']\n",
    "    rucio_data['delay'] = rucio_data['delay'].astype('timedelta64[s]')\n",
    "    \n",
    "    rucio_data = rucio_data.sort_values(by='submitted_at')\n",
    "    \n",
    "    # Reindex data with 'submitted_at timestamp'\n",
    "    rucio_data.index = pd.DatetimeIndex(rucio_data['submitted_at'])\n",
    "    \n",
    "    #remove all timestamp columns\n",
    "    rucio_data = rucio_data.drop(timestamps, axis=1)\n",
    "    \n",
    "    # encode categorical data\n",
    " \n",
    "    if use_cache==True:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=True)\n",
    "    else:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=False)\n",
    "\n",
    "    rucio_data['src-rse'] = src_encoder.transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rescale_data(rucio_data, durations):\n",
    "    # Normalization\n",
    "    # using custom scaling parameters (based on trends of the following variables)\n",
    "\n",
    "    durations = durations / 1e3\n",
    "    rucio_data['bytes'] = rucio_data['bytes'] / 1e10\n",
    "    rucio_data['delay'] = rucio_data['delay'] / 1e5\n",
    "    rucio_data['src-rse'] = rucio_data['src-rse'] / 1e2\n",
    "    rucio_data['dst-rse'] = rucio_data['dst-rse'] / 1e2\n",
    "    \n",
    "    return rucio_data, durations\n",
    "\n",
    "def plot_graphs_and_rescale(data):\n",
    "    \n",
    "    durations = data['duration']\n",
    "    durations.plot()\n",
    "    plt.ylabel('durations(seconds)')\n",
    "    plt.show()\n",
    "\n",
    "    filesize = data['bytes']\n",
    "    filesize.plot(label='filesize(bytes)')\n",
    "    plt.ylabel('bytes')\n",
    "    plt.show()\n",
    "\n",
    "    delays = data['delay']\n",
    "    delays.plot(label='delay(seconds)')\n",
    "    plt.ylabel('delay')\n",
    "    plt.show()\n",
    "    \n",
    "    print('rescaling input continuous variables : filesizes, queue-times, transfer-durations')\n",
    "    data, byte_scaler, delay_scaler, duration_scaler = rescale_data(data)\n",
    "\n",
    "    plt.plot(data['bytes'], 'r', label='filesize')\n",
    "    plt.plot(data['duration'], 'y', label='durations')\n",
    "    plt.plot(data['delay'],'g', label='queue-time')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return data, byte_scaler, delay_scaler, duration_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_inputs(rucio_data,durations, num_timesteps=50):\n",
    "    \n",
    "    #slice_size = batch_size*num_timesteps\n",
    "    print(rucio_data.shape[0], durations.shape)\n",
    "    n_examples = rucio_data.shape[0]\n",
    "    n_batches = (n_examples - num_timesteps +1)\n",
    "    print('Total Data points for training/testing : {} of {} timesteps each.'.format(n_batches, num_timesteps))\n",
    "    \n",
    "    inputs=[]\n",
    "    outputs=[]\n",
    "    for i in range(0,n_batches):\n",
    "        v = rucio_data[i:i+num_timesteps]\n",
    "        w = durations[i+num_timesteps-1]\n",
    "        inputs.append(v)\n",
    "        outputs.append(w)\n",
    "    inputs = np.stack(inputs)\n",
    "    outputs = np.stack(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = '../' # Change this as you need.\n",
    "\n",
    "def get_rucio_files(path='../', n_files =100):\n",
    "    abspaths = []\n",
    "    for fn in os.listdir(path):\n",
    "        if 'atlas_rucio' in fn:\n",
    "            abspaths.append(os.path.abspath(os.path.join(path, fn)))\n",
    "    print(\"\\n Found : \".join(abspaths))\n",
    "    print('\\n total files found = {}'.format(len(abspaths)))\n",
    "    return abspaths\n",
    "\n",
    "def load_rucio_data(file, use_cache = True, limit=None):\n",
    "    print('reading : {}'.format(file))\n",
    "    data = pd.read_csv(file)\n",
    "    if limit != None:\n",
    "        data= data[950000: 950000+limit]\n",
    "        print('Limiting data size to {} '.format(limit))\n",
    "#     print(data)\n",
    "    print('preprocessing data... ')\n",
    "    data = preprocess_data(data)\n",
    "    print('Saving indices for later..')\n",
    "    indices = data.index\n",
    "    durations = data['duration']\n",
    "    data = data.drop(['duration'], axis=1)\n",
    "    data = data[['bytes', 'delay', 'activity', 'dst-rse', 'dst-type',\n",
    "                 'protocol', 'src-rse', 'src-type', 'transfer-endpoint']]\n",
    "    data, durations = rescale_data(data, durations)\n",
    "    data = data.as_matrix()\n",
    "    durations = durations.as_matrix()\n",
    "    return data, durations, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path='data/'\n",
    "# a= get_rucio_files(path=path)\n",
    "# x, y, indices = load_rucio_data(a[1], limit=5)\n",
    "\n",
    "# print(x ,'\\n', y, '\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x,y = prepare_model_inputs(x,y,num_timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def return_to_original(x, y, preds, index=None):\n",
    "    y = y * 1e3\n",
    "    pred = pred * 1e3\n",
    "    #print(x.shape, y.shape)\n",
    "    #print(x[0,1])\n",
    "    n_steps = x.shape[1]\n",
    "    #print(index[:n_steps])\n",
    "    #print(index[n_steps-1:])\n",
    "    index = index[n_steps-1:]\n",
    "    \n",
    "    cols = ['bytes', 'delay', 'activity', 'dst-rse', 'dst-type','protocol', 'src-rse', 'src-type', 'transfer-endpoint']\n",
    "    data = list(x[0])\n",
    "    for i in range(1,x.shape[0]):\n",
    "        data.append(x[i,n_steps-1,:])\n",
    "    \n",
    "    data = data[n_steps-1:]\n",
    "    #print(len(data))\n",
    "    data = pd.DataFrame(data, index=index, columns=cols)\n",
    "    data['bytes'] = data['bytes'] * 1e10\n",
    "    data['delay'] = data['delay'] * 1e5\n",
    "    data['src-rse'] = data['src-rse'] * 1e2\n",
    "    data['dst-rse'] = data['dst-rse'] * 1e2\n",
    "    \n",
    "    data = data.round().astype(int)\n",
    "    print(data.shape)\n",
    "    data = decode_labels(data)\n",
    "    data['duration'] =y\n",
    "    data['prediction'] = pred\n",
    "    return data\n",
    "\n",
    "# return_to_original(x,y, index=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_labels(rucio_data):\n",
    "    src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = load_encoders()\n",
    "    \n",
    "    rucio_data['src-rse'] = src_encoder.inverse_transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.inverse_transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.inverse_transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.inverse_transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.inverse_transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.inverse_transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.inverse_transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data\n",
    "\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model(num_timesteps=50, batch_size = 512, parallel=False):\n",
    "\n",
    "    model = Sequential()\n",
    "    layers = [512, 512, 512, 512, 128, 1]\n",
    "    \n",
    "    model.add(LSTM(layers[0], input_shape=(num_timesteps, 9), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[1], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[2], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(layers[3]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[4]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[5]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if parallel:\n",
    "        model = make_parallel(model,4)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    sns.set_context('poster')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    print(len(losses))\n",
    "    fig.show()\n",
    "\n",
    "def train_network(model=None,limit=None, data=None, epochs=1,n_timesteps=100, batch=128, path=\"data/\",parallel=True):\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_model(num_timesteps=n_timesteps, parallel=parallel)\n",
    "        history = LossHistory()\n",
    "            \n",
    "        checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "        print('model built and compiled !')\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "    try:\n",
    "        for i,file in enumerate(a):\n",
    "            print(\"Training on file :{}\".format(file))\n",
    "            x, y, indices = load_rucio_data(file, limit=limit)\n",
    "            print('\\n Data Loaded and preprocessed !!....')\n",
    "            x, y = prepare_model_inputs(x, y, num_timesteps=n_timesteps)\n",
    "            print('Data ready for training.')\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training model...')\n",
    "            if parallel:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch*4,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "            else:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "\n",
    "            print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "            score = model.evaluate(x, y, verbose=0)\n",
    "            print(\"Network's Residual training score [MSE]: {0} ; [in seconds]: {1}\".format(score,np.sqrt(score)))\n",
    "            print(\"Training on {} finished !!\".format(file))\n",
    "            print('\\n Saving model to disk..')\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/lstm_model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/lstm_model.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            print('plotting losses..')\n",
    "            plot_losses(history.losses)\n",
    "\n",
    "        print('Training Complete !!')\n",
    "        \n",
    "        return training, model, indices, history.losses\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            return model, history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_network(n_timesteps=100, batch=256, parallel =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_lstm():\n",
    "    json_file = open('models/lstm_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"models/lstm_model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print('Model model compiled!!')\n",
    "    return loaded_model\n",
    "\n",
    "def evaluate_network(limit=None, n_timesteps=100, path=\"data/\",model=None):\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "\n",
    "    for i,file in enumerate(a):\n",
    "        print(\"Training on file :{}\".format(file))\n",
    "        x, y, indices = load_rucio_data(file, limit=limit)\n",
    "        print('\\n Data Loaded and preprocessed !!....')\n",
    "        x, y = prepare_model_inputs(x, y, num_timesteps=n_timesteps)\n",
    "        print('Data ready for Evaluation')\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            start_time = time.time()\n",
    "            print('making predictions...')\n",
    "            model = load_lstm()\n",
    "            predictions = model.predict(x)\n",
    "            end = time.time - start_time\n",
    "            print('Done !! in {} min'.format(end/60))\n",
    "            print('plotting graphs')\n",
    "\n",
    "            plt.plot(y, 'g')\n",
    "            plt.plot(predictions, 'y')\n",
    "            plt.show()\n",
    "\n",
    "            data = return_to_original(x, y, predictions, index=indices)\n",
    "            plt.plot(data['duration'], 'g')\n",
    "            plt.plot(data['prediction'], 'y')\n",
    "            plt.title('Network predictions')\n",
    "            plt.ylabel('durations in seconds')\n",
    "            plt.show()\n",
    "\n",
    "            data['mae'] = data['duration'] - data['prediction']\n",
    "            data['mae'].plot()\n",
    "            plt.show()\n",
    "#             if 'model_pred' not in os.listdir():\n",
    "#                 os.mkdir('model_pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Locating training data files...\n",
      "/home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.09.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.17.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.13.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.22.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.21.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.01.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.03.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.20.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.11.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.05.csv\n",
      "\n",
      " total files found = 10\n",
      "Training on file :/home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.09.csv\n",
      "reading : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.09.csv\n",
      "Limiting data size to 50000 \n",
      "preprocessing data... \n",
      "using cached LabelEncoders for encoding data.....\n",
      "Saving indices for later..\n",
      "\n",
      " Data Loaded and preprocessed !!....\n",
      "50000 (50000,)\n",
      "Total Data points for training/testing : 49901 of 100 timesteps each.\n",
      "(49901, 100, 9) (49901,)\n",
      "Data ready for Evaluation\n",
      "making predictions...\n",
      "Loaded model from disk\n",
      "Model model compiled!!\n"
     ]
    }
   ],
   "source": [
    "evaluate_network(path='data/', limit = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.09.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.17.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.13.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.22.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.21.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.01.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.03.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.20.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.11.csv\n",
      " Found : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.05.csv\n",
      "\n",
      " total files found = 10\n",
      "reading : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.13.csv\n",
      "Limiting data size to 1000 \n",
      "preprocessing data... \n",
      "using cached LabelEncoders for encoding data.....\n",
      "Saving indices for later..\n",
      "\n",
      " Data Loaded and preprocessed !!....\n",
      "1000 (1000,)\n",
      "Total Data points for training/testing : 901 of 100 timesteps each.\n",
      "(901, 100, 9) (901,)\n",
      "Loaded model from disk\n",
      "Model model compiled!!\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# a= get_rucio_files(path=path)\n",
    "# x, y, indices = load_rucio_data(a[2], limit=1000)\n",
    "# print('\\n Data Loaded and preprocessed !!....')\n",
    "# x, y = prepare_model_inputs(x, y, num_timesteps=100)\n",
    "# with tf.device('/gpu:0'):\n",
    "#     model = load_lstm()\n",
    "#     pred= model.predict(x)\n",
    "#     print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f71e6f13e48>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f71faf4b240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
