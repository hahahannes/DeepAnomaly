{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model,model_from_json\n",
    "from keras.layers import Dense,Activation,Dropout,Input\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import Adam\n",
    "import keras.callbacks as cb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_encoders():\n",
    "    src_encoder = LabelEncoder()\n",
    "    dst_encoder = LabelEncoder()\n",
    "    type_encoder = LabelEncoder()\n",
    "    activity_encoder = LabelEncoder()\n",
    "    protocol_encoder = LabelEncoder()\n",
    "    t_endpoint_encoder = LabelEncoder()\n",
    "    \n",
    "    src_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    dst_encoder.classes_ = np.load('encoders/ddm_rse_endpoints.npy')\n",
    "    type_encoder.classes_ = np.load('encoders/type.npy')\n",
    "    activity_encoder.classes_ = np.load('encoders/activity.npy')\n",
    "    protocol_encoder.classes_ = np.load('encoders/protocol.npy')\n",
    "    t_endpoint_encoder.classes_ = np.load('encoders/endpoint.npy')\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n",
    "\n",
    "def train_encoders(rucio_data, use_cache=True):\n",
    "    \n",
    "    if use_cache:\n",
    "        if os.path.isfile('encoders/ddm_rse_endpoints.npy') and os.path.isfile('encoders/activity.npy'):\n",
    "            print('using cached LabelEncoders for encoding data.....')\n",
    "            src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder=load_encoders()\n",
    "        else:\n",
    "            print('NO cache found')\n",
    "    else:\n",
    "        print('No cached encoders found ! Training Some New Ones using input data!')\n",
    "        src_encoder = LabelEncoder()\n",
    "        dst_encoder = LabelEncoder()\n",
    "        type_encoder = LabelEncoder()\n",
    "        activity_encoder = LabelEncoder()\n",
    "        protocol_encoder = LabelEncoder()\n",
    "        t_endpoint_encoder = LabelEncoder()\n",
    "\n",
    "        src_encoder.fit(rucio_data['src-rse'].unique())\n",
    "        dst_encoder.fit(rucio_data['dst-rse'].unique())\n",
    "        type_encoder.fit(rucio_data['src-type'].unique())\n",
    "        activity_encoder.fit(rucio_data['activity'].unique())\n",
    "        protocol_encoder.fit(rucio_data['protocol'].unique())\n",
    "        t_endpoint_encoder.fit(rucio_data['transfer-endpoint'].unique())\n",
    "\n",
    "        np.save('encoders/src.npy', src_encoder.classes_)\n",
    "        np.save('encoders/dst.npy', dst_encoder.classes_)\n",
    "        np.save('encoders/type.npy', type_encoder.classes_)\n",
    "        np.save('encoders/activity.npy', activity_encoder.classes_)\n",
    "        np.save('encoders/protocol.npy', protocol_encoder.classes_)\n",
    "        np.save('encoders/endpoint.npy', t_endpoint_encoder.classes_)\n",
    "    \n",
    "    return (src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(rucio_data, use_cache=True):\n",
    "    \n",
    "    fields_to_drop = ['account','reason','checksum-adler','checksum-md5','guid','request-id','transfer-id','tool-id',\n",
    "                      'transfer-link','name','previous-request-id','scope','src-url','dst-url', 'Unnamed: 0']\n",
    "    timestamps = ['started_at', 'submitted_at','transferred_at']\n",
    "\n",
    "    #DROP FIELDS , CHANGE TIME FORMAT, add dataetime index\n",
    "    rucio_data = rucio_data.drop(fields_to_drop, axis=1)\n",
    "    for timestamp in timestamps:\n",
    "        rucio_data[timestamp]= pd.to_datetime(rucio_data[timestamp], infer_datetime_format=True)\n",
    "    rucio_data['delay'] = rucio_data['started_at'] - rucio_data['submitted_at']\n",
    "    rucio_data['delay'] = rucio_data['delay'].astype('timedelta64[s]')\n",
    "    \n",
    "    rucio_data = rucio_data.sort_values(by='submitted_at')\n",
    "    \n",
    "    # Reindex data with 'submitted_at timestamp'\n",
    "    rucio_data.index = pd.DatetimeIndex(rucio_data['submitted_at'])\n",
    "    \n",
    "    #remove all timestamp columns\n",
    "    rucio_data = rucio_data.drop(timestamps, axis=1)\n",
    "    \n",
    "    # encode categorical data\n",
    " \n",
    "    if use_cache==True:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=True)\n",
    "    else:\n",
    "        src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = train_encoders(rucio_data, use_cache=False)\n",
    "\n",
    "    rucio_data['src-rse'] = src_encoder.transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rescale_data(rucio_data, durations):\n",
    "    # Normalization\n",
    "    # using custom scaling parameters (based on trends of the following variables)\n",
    "\n",
    "    durations = durations / 1e3\n",
    "    rucio_data['bytes'] = rucio_data['bytes'] / 1e10\n",
    "    rucio_data['delay'] = rucio_data['delay'] / 1e5\n",
    "    rucio_data['src-rse'] = rucio_data['src-rse'] / 1e2\n",
    "    rucio_data['dst-rse'] = rucio_data['dst-rse'] / 1e2\n",
    "    \n",
    "    return rucio_data, durations\n",
    "\n",
    "def plot_graphs_and_rescale(data):\n",
    "    \n",
    "    durations = data['duration']\n",
    "    durations.plot()\n",
    "    plt.ylabel('durations(seconds)')\n",
    "    plt.show()\n",
    "\n",
    "    filesize = data['bytes']\n",
    "    filesize.plot(label='filesize(bytes)')\n",
    "    plt.ylabel('bytes')\n",
    "    plt.show()\n",
    "\n",
    "    delays = data['delay']\n",
    "    delays.plot(label='delay(seconds)')\n",
    "    plt.ylabel('delay')\n",
    "    plt.show()\n",
    "    \n",
    "    print('rescaling input continuous variables : filesizes, queue-times, transfer-durations')\n",
    "    data, byte_scaler, delay_scaler, duration_scaler = rescale_data(data)\n",
    "\n",
    "    plt.plot(data['bytes'], 'r', label='filesize')\n",
    "    plt.plot(data['duration'], 'y', label='durations')\n",
    "    plt.plot(data['delay'],'g', label='queue-time')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return data, byte_scaler, delay_scaler, duration_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_inputs(rucio_data,durations, num_timesteps=50):\n",
    "    \n",
    "    #slice_size = batch_size*num_timesteps\n",
    "    print(rucio_data.shape[0], durations.shape)\n",
    "    n_examples = rucio_data.shape[0]\n",
    "    n_batches = (n_examples - num_timesteps +1)\n",
    "    print('Total Data points for training/testing : {} of {} timesteps each.'.format(n_batches, num_timesteps))\n",
    "    \n",
    "    inputs=[]\n",
    "    outputs=[]\n",
    "    for i in range(0,n_batches):\n",
    "        v = rucio_data[i:i+num_timesteps]\n",
    "        w = durations[i+num_timesteps-1]\n",
    "        inputs.append(v)\n",
    "        outputs.append(w)\n",
    "    inputs = np.stack(inputs)\n",
    "    outputs = np.stack(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = '../' # Change this as you need.\n",
    "\n",
    "def get_rucio_files(path='../', n_files =100):\n",
    "    abspaths = []\n",
    "    for fn in os.listdir(path):\n",
    "        if 'atlas_rucio' in fn:\n",
    "            abspaths.append(os.path.abspath(os.path.join(path, fn)))\n",
    "    print(\"\\n Found : \".join(abspaths))\n",
    "    print('\\n total files found = {}'.format(len(abspaths)))\n",
    "    return abspaths\n",
    "\n",
    "def load_rucio_data(file, use_cache = True, limit=None):\n",
    "    print('reading : {}'.format(file))\n",
    "    data = pd.read_csv(file)\n",
    "    if limit != None:\n",
    "        data= data[:limit]\n",
    "        print('Limiting data size to {} '.format(limit))\n",
    "#     print(data)\n",
    "    print('preprocessing data... ')\n",
    "    data = preprocess_data(data)\n",
    "    print('Saving indices for later..')\n",
    "    indices = data.index\n",
    "    durations = data['duration']\n",
    "    data = data.drop(['duration'], axis=1)\n",
    "    data = data[['bytes', 'delay', 'activity', 'dst-rse', 'dst-type',\n",
    "                 'protocol', 'src-rse', 'src-type', 'transfer-endpoint']]\n",
    "    data, durations = rescale_data(data, durations)\n",
    "    data = data.as_matrix()\n",
    "    durations = durations.as_matrix()\n",
    "    return data, durations, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a= get_rucio_files(path=path)\n",
    "# x, y, indices = load_rucio_data(a[12], limit=5)\n",
    "\n",
    "# print(x ,'\\n', y, '\\n', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# x,y = prepare_model_inputs(x,y,num_timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def return_to_original(x, y, index=None):\n",
    "    y = y * 1e3\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[0])\n",
    "    cols = ['bytes', 'delay', 'activity', 'dst-rse', 'dst-type','protocol', 'src-rse', 'src-type', 'transfer-endpoint']\n",
    "    n_steps = x.shape[1]\n",
    "    data = list(x[0])\n",
    "    for i in range(1,x.shape[0]):\n",
    "        data.append(x[i,n_steps-1,:])\n",
    "    data = pd.DataFrame(data, index=indices, columns=cols)\n",
    "    data['bytes'] = data['bytes'] * 1e10\n",
    "    data['delay'] = data['delay'] * 1e5\n",
    "    data['src-rse'] = data['src-rse'] * 1e2\n",
    "    data['dst-rse'] = data['dst-rse'] * 1e2\n",
    "    \n",
    "    data = data.round().astype(int)\n",
    "    data = decode_labels(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decode_labels(rucio_data):\n",
    "    src_encoder,dst_encoder,type_encoder,activity_encoder,protocol_encoder,t_endpoint_encoder = load_encoders()\n",
    "    \n",
    "    rucio_data['src-rse'] = src_encoder.inverse_transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.inverse_transform(rucio_data['dst-rse'])\n",
    "    rucio_data['src-type'] = type_encoder.inverse_transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.inverse_transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.inverse_transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.inverse_transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.inverse_transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data\n",
    "\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "            \n",
    "        return Model(input=model.inputs, output=merged)\n",
    "\n",
    "def build_model(num_timesteps=50, batch_size = 512, parallel=False):\n",
    "\n",
    "    model = Sequential()\n",
    "    layers = [512, 512, 512, 512, 128, 1]\n",
    "    \n",
    "    model.add(LSTM(layers[0], input_shape=(num_timesteps, 9), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[1], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(layers[2], return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(layers[3]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[4]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Dense(layers[5]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if parallel:\n",
    "        model = make_parallel(model,4)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    sns.set_context('poster')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    print(len(losses))\n",
    "    fig.show()\n",
    "\n",
    "def train_network(model=None,limit=None, data=None, epochs=1,n_timesteps=100, batch=128, path=\"data/\",parallel=True):\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_model(num_timesteps=n_timesteps, parallel=parallel)\n",
    "        history = LossHistory()\n",
    "            \n",
    "        checkpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "        print('model built and compiled !')\n",
    "    \n",
    "    print('\\n Locating training data files...')\n",
    "    a= get_rucio_files(path=path)\n",
    "    \n",
    "    try:\n",
    "        for i,file in enumerate(a):\n",
    "            print(\"Training on file :{}\".format(file))\n",
    "            x, y, indices = load_rucio_data(file, limit=limit)\n",
    "            print('\\n Data Loaded and preprocessed !!....')\n",
    "            x, y = prepare_model_inputs(x, y, num_timesteps=n_timesteps)\n",
    "            print('Data ready for training.')\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            print('Training model...')\n",
    "            if parallel:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch*4,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "            else:\n",
    "                training = model.fit(x, y, epochs=epochs, batch_size=batch,\n",
    "                                     validation_split=0.1, callbacks=[history,TQDMNotebookCallback(leave_inner=True), checkpointer],\n",
    "                                     verbose=0)\n",
    "\n",
    "            print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "            score = model.evaluate(x, y, verbose=0)\n",
    "            print(\"Network's Residual training score [MSE]: {0} ; [in seconds]: {1}\".format(score,np.sqrt(score)))\n",
    "            print(\"Training on {} finished !!\".format(file))\n",
    "            print('\\n Saving model to disk..')\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/lstm_model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/lstm_model.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "            print('plotting losses..')\n",
    "            plot_losses(history.losses)\n",
    "\n",
    "        print('Training Complete !!')\n",
    "        \n",
    "        return training, model, indices, history.losses\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            return model, history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.02346491813659668\n",
      "model built and compiled !\n",
      "\n",
      " Locating training data files...\n",
      "/home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.01.csv\n",
      "\n",
      " total files found = 1\n",
      "Training on file :/home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.01.csv\n",
      "reading : /home/carnd/DeepAnomaly/data/atlas_rucio-events-2017.06.01.csv\n",
      "preprocessing data... \n",
      "using cached LabelEncoders for encoding data.....\n",
      "Saving indices for later..\n",
      "\n",
      " Data Loaded and preprocessed !!....\n",
      "1862550 (1862550,)\n",
      "Total Data points for training/testing : 1862451 of 100 timesteps each.\n",
      "(1862451, 100, 9) (1862451,)\n",
      "Data ready for training.\n",
      "Training model...\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.models.Sequential at 0x7f26ff8347f0>,\n",
       " [0.042013686,\n",
       "  0.42684162,\n",
       "  0.085903458,\n",
       "  0.032871731,\n",
       "  0.02270175,\n",
       "  0.040394083,\n",
       "  0.035752945,\n",
       "  0.024653362,\n",
       "  0.025593879,\n",
       "  0.033706125,\n",
       "  0.0088486588,\n",
       "  0.013900356,\n",
       "  0.043930318,\n",
       "  0.022239819,\n",
       "  0.023631897,\n",
       "  0.072096534,\n",
       "  0.011476594,\n",
       "  0.0063070059,\n",
       "  0.035965055,\n",
       "  0.020178894,\n",
       "  0.16987823,\n",
       "  0.0099933986,\n",
       "  0.01916779,\n",
       "  0.017211653,\n",
       "  0.017238626,\n",
       "  0.0062889229,\n",
       "  0.036488481,\n",
       "  0.0072311228,\n",
       "  0.015430009,\n",
       "  0.016809821,\n",
       "  0.0077547538,\n",
       "  0.0084657585,\n",
       "  0.0070805196,\n",
       "  0.0083353808,\n",
       "  0.051854409,\n",
       "  0.0064960476,\n",
       "  0.0064842035,\n",
       "  0.13942373,\n",
       "  0.05365609,\n",
       "  0.026418591,\n",
       "  0.19256626,\n",
       "  0.023599874,\n",
       "  0.0071154935,\n",
       "  0.016715745,\n",
       "  0.0056812186,\n",
       "  0.020989738,\n",
       "  0.0090984087,\n",
       "  0.022005277,\n",
       "  0.01799559])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_network(n_timesteps=100, batch=256, parallel =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "02c65874399f4988aab8cc3bcafcb217": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "0672b3ccd73b45278816d3c8e989aae7": {
     "views": []
    },
    "0a60ecd1e1734abc9a9887313fb94fb0": {
     "views": []
    },
    "0b5c9ee16ba245608040542f08ac1661": {
     "views": []
    },
    "11f7d93c55ce48c786c6c780f92af437": {
     "views": []
    },
    "16f596e577ce4d7d8399870d847f5805": {
     "views": []
    },
    "19a700ecd8f344f2b0deebc57b4b62cb": {
     "views": []
    },
    "1a31420af4734014a494c7e943ebaad4": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "23fcd937a5a04d3ba5261982e5922233": {
     "views": []
    },
    "3317fc2fae444a52837d2307f95c7c73": {
     "views": []
    },
    "3ed6a7195adc4f91be3ad47f2443e17f": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "3f793a59e4e6461a8d4b0ba047270afa": {
     "views": []
    },
    "4a7fe7c966884509874f8d5612bda391": {
     "views": []
    },
    "5187bb5d0d2e4bc2b601c6040c057659": {
     "views": []
    },
    "522951e00ca74737abaa3309ac5574da": {
     "views": []
    },
    "5ebf4d59cee04785aee28c9343328091": {
     "views": []
    },
    "6ff8cc07310c42ff832d66ea03909b6e": {
     "views": []
    },
    "730f28fddb824c308680451d252370ce": {
     "views": []
    },
    "78adefd80997488da230d32e4c75121f": {
     "views": []
    },
    "79d243e2a696458dacfb2ccc6dfc5494": {
     "views": []
    },
    "7ab3474970cf4f54ab12fd04926a9938": {
     "views": []
    },
    "7c1382fb7a834944a275cdf07a89bccb": {
     "views": []
    },
    "81b0ec1693044eeea45a6cbe006b1384": {
     "views": []
    },
    "84a0c319941f4e1fa87807773085c2b2": {
     "views": []
    },
    "8fbdf0da0cde4b03b605174826ed119d": {
     "views": []
    },
    "90fbe13825eb45a4ba53ed111b6ad7ab": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "950fe28fb40348359fb4ccbe43ac44ba": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "9f7aa3a51def4db897bf52ec5beb3033": {
     "views": []
    },
    "a2eef33f6ee74ba7b269b90721a12a5a": {
     "views": []
    },
    "a750e61d8407454d92236ec905cc51ce": {
     "views": []
    },
    "a84230427a8541a3bf54def41d8b58b0": {
     "views": []
    },
    "ada0e54b76af4f2c838d0324c9096f2a": {
     "views": []
    },
    "ae8943a701014f5ebc7e3c4b873af9d6": {
     "views": []
    },
    "b2207990dd84495f8f9a0bbeb5dbdba7": {
     "views": []
    },
    "b5a4785d483b4a0b8279f0db77876140": {
     "views": []
    },
    "b7219d1df971475bb72fd6b3d26ebcb5": {
     "views": []
    },
    "b8fdbb5c416448659e359d24a6609abf": {
     "views": []
    },
    "b9dc1bb1f05a4d208968e00bbb559d31": {
     "views": []
    },
    "c0eaeeb58dd846f6bda7f723f77e9990": {
     "views": []
    },
    "c6e0cbd37aa74ab99f66ed8019285bfc": {
     "views": []
    },
    "d13da964e21e4d6bb0a8650d69d58672": {
     "views": []
    },
    "d2eae25c0cdb4b91b3b436b9b9fba1c0": {
     "views": []
    },
    "dc843a55fea4427c824d54782c0b94a3": {
     "views": []
    },
    "e22dd6f2dc9c4c098c624a7a6e379e93": {
     "views": []
    },
    "e736e1a1649947549fa3b1abb0dfa558": {
     "views": []
    },
    "e7cab847e6904f09a3880980a3b52ee8": {
     "views": []
    },
    "ea8a18a5aebf4f83a1d46fc3df24cc5d": {
     "views": []
    },
    "f6599f39bb6d498bbcfe5f3284b6f6ce": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
